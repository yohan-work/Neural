# 인공신경망(Neural Network) 기초 개념

## 1. 뉴런 (Neuron) / 퍼셉트론 (Perceptron)
인공신경망의 가장 기본 단위. 생물학적 뉴런을 모방.
- **입력 (Input)**: 외부로부터 들어오는 신호 ($x_1, x_2, ...$)
- **가중치 (Weight)**: 각 입력 신호의 중요도 ($w_1, w_2, ...$). 학습을 통해 이 값들이 조정.
- **편향 (Bias)**: 뉴런이 얼마나 쉽게 활성화될지를 결정하는 값 ($b$).
- **가중합 (Weighted Sum)**: 입력과 가중치를 곱하고 편향을 더한 값 ($z = \sum x_i w_i + b$).
- **활성화 함수 (Activation Function)**: 가중합을 통과시켜 최종 출력값을 결정하는 함수. 비선형성을 추가하는 핵심 역할.

## 2. 레이어 (Layers)
뉴런들이 모여 층을 이룬 구조.
- **입력층 (Input Layer)**: 데이터가 처음 들어오는 곳. 아무런 연산도 수행하지 않고 값만 전달.
- **은닉층 (Hidden Layer)**: 입력층과 출력층 사이에 있는 층. 복잡한 패턴을 학습하는 곳. '딥러닝'의 '딥(Deep)'은 이 은닉층이 많다는 뜻.
- **출력층 (Output Layer)**: 최종 결과를 내보내는 층. 문제의 종류(분류, 회귀 등)에 따라 다른 활성화 함수 사용.

## 3. 활성화 함수 (Activation Functions)
신경망에 비선형성(Non-linearity)을 부여하여 복잡한 문제를 풀 수 있게 해줍니다.
- **Sigmoid**: 0과 1 사이의 값을 출력. 이진 분류(Binary Classification)의 출력층에 주로 사용.
- **ReLU (Rectified Linear Unit)**: 0보다 작으면 0, 크면 그대로 출력. 은닉층에서 가장 많이 사용. 학습을 빠르게 하고 기울기 소실 문제를 완화.
