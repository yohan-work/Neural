import cv2
import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image
import os
import time
import mediapipe as mp

# 1. ë””ë°”ì´ìŠ¤ ì„¤ì •
if torch.backends.mps.is_available():
    device = torch.device("mps")
elif torch.cuda.is_available():
    device = torch.device("cuda")
else:
    device = torch.device("cpu")

# 2. ëª¨ë¸ ì •ì˜ (í•™ìŠµ ë•Œì™€ ë™ì¼)
class MNIST_CNN(nn.Module):
    def __init__(self):
        super(MNIST_CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.pool(x)
        x = self.conv2(x)
        x = self.relu(x)
        x = self.pool(x)
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x

class EMNIST_CNN(nn.Module):
    def __init__(self):
        super(EMNIST_CNN, self).__init__()
        # êµ¬ì¡°ëŠ” MNISTì™€ ë™ì¼í•˜ê²Œ ìœ ì§€í•˜ë˜, Outputë§Œ ë³€ê²½
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(128, 26) # 26 Alphabets (0-25)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.pool(x)
        x = self.conv2(x)
        x = self.relu(x)
        x = self.pool(x)
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x

def main():
    # ëª¨ë¸ ë¡œë“œ (Digit & Alphabet)
    digit_model_path = './mnist_cnn.pth'
    alpha_model_path = './emnist_cnn.pth'
    
    digit_model = MNIST_CNN().to(device)
    alpha_model = EMNIST_CNN().to(device)
    
    models_loaded = {"digit": False, "alpha": False}

    if os.path.exists(digit_model_path):
        try:
            digit_model.load_state_dict(torch.load(digit_model_path, map_location=device))
            digit_model.eval()
            models_loaded["digit"] = True
            print("Digit model loaded.")
        except:
            print("Failed to load digit model.")
    
    if os.path.exists(alpha_model_path):
        try:
            alpha_model.load_state_dict(torch.load(alpha_model_path, map_location=device))
            alpha_model.eval()
            models_loaded["alpha"] = True
            print("Alphabet model loaded.")
        except:
             print("Failed to load alphabet model.")

    if not models_loaded["digit"] and not models_loaded["alpha"]:
        print("Error: No models found. Please train at least one model.")
        return

    # ì „ì²˜ë¦¬ ë³€í™˜
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    
    # EMNISTìš© ì „ì²˜ë¦¬ (Rotate & Flip)
    # í•™ìŠµ ë•Œì™€ ë‹¬ë¦¬, ì´ë¯¸ PIL Imageë¡œ ë“¤ì–´ì˜¤ëŠ” í•¸ë“œ ë“œë¡œì‰ ì´ë¯¸ì§€ëŠ” ì •ë°©í–¥ì´ë¯€ë¡œ
    # ëª¨ë¸ì´ í•™ìŠµëœ ë°©ì‹(90ë„ íšŒì „ëœ ë°ì´í„°)ì— ë§ì¶°ì„œ ëŒë ¤ì¤˜ì„œ ë„£ì–´ì¤˜ì•¼ í•¨.
    # í•™ìŠµ ë°ì´í„°: 90ë„ íšŒì „ë˜ì–´ ìˆìŒ -> ëª¨ë¸ì´ ê·¸ê±¸ í•™ìŠµí•¨.
    # ì…ë ¥ ë°ì´í„°: ì •ë°©í–¥ -> ëª¨ë¸ì— ë„£ì„ ë•Œ 90ë„ íšŒì „í•´ì„œ ë„£ì–´ì•¼ ë§¤ì¹­ë¨.
    emnist_transform_fn = transforms.Compose([
        transforms.functional.hflip,
        lambda x: transforms.functional.rotate(x, -90), # -90 or 90 depends on how data was loaded. Usually EMNIST raw is rotated.
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])

    # MediaPipe Hands ì´ˆê¸°í™”
    mp_hands = mp.solutions.hands
    hands = mp_hands.Hands(
        max_num_hands=1,
        min_detection_confidence=0.7,
        min_tracking_confidence=0.7
    )
    mp_draw = mp.solutions.drawing_utils

    # ì›¹ìº  ì„¤ì •
    cap = cv2.VideoCapture(0)
    
    # ìº”ë²„ìŠ¤ ì´ˆê¸°í™”
    canvas = None
    
    # ì œìŠ¤ì²˜ ì¿¨ë‹¤ìš´ ë° ìƒíƒœ ë³€ìˆ˜
    last_action_time = 0
    cooldown = 1.0 # 1ì´ˆ ì¿¨ë‹¤ìš´
    
    # íŒ ID (ì—„ì§€, ê²€ì§€, ì¤‘ì§€, ì•½ì§€, ì†Œì§€)
    # MediaPipe Hand Landmarks: 
    # 4: Thumb Tip, 8: Index Tip, 12: Middle Tip, 16: Ring Tip, 20: Pinky Tip
    tip_ids = [4, 8, 12, 16, 20]

    # ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
    x1, y1 = 0, 0
    prediction_text = ""
    current_mode = "digit" # 'digit' or 'alpha'
    alphabet_map = {i: chr(65+i) for i in range(26)}

    print("Controls (Gestures):")
    print(" - â˜ï¸  Index Up: DRAW")
    print(" - âœŒï¸  Index + Middle Up: HOVER (Move without drawing)")
    print(" - âœŠ  Fist (All Down): CLEAR")
    print(" - ğŸ‘  Thumb Up (Only): PREDICT")
    print(" - ğŸ¤˜  Rock (Index + Pinky): SWITCH MODE")
    print(" - 'q' to QUIT")

    while True:
        ret, frame = cap.read()
        if not ret:
            break
            
        frame = cv2.flip(frame, 1) # ê±°ìš¸ ëª¨ë“œ
        height, width, _ = frame.shape
        
        if canvas is None:
            canvas = np.zeros_like(frame)

        img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        result = hands.process(img_rgb)
        
        gesture_name = "None"
        
        if result.multi_hand_landmarks:
            for hand_landmarks in result.multi_hand_landmarks:
                mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
                
                # ì†ê°€ë½ ìƒíƒœ íŒë³„
                fingers = []
                
                # ì—„ì§€: xì¢Œí‘œ ë¹„êµ (ì˜¤ë¥¸ì† ê¸°ì¤€, ê±°ìš¸ ëª¨ë“œë¼ ë°˜ëŒ€ì¼ ìˆ˜ ìˆìŒ í™•ì¸ í•„ìš”)
                # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì—„ì§€ íŒì´ ì—„ì§€ ê´€ì ˆ(IP, 3)ë³´ë‹¤ ë°”ê¹¥ìª½ì— ìˆëŠ”ì§€ë³´ë‹¤ëŠ” 
                # ë‹¨ìˆœíˆ yì¢Œí‘œë‚˜ xì¢Œí‘œ ìƒëŒ€ ìœ„ì¹˜ë¡œ í•´ì•¼í•˜ëŠ”ë°, ì—„ì§€ëŠ” íšŒì „ì´ ììœ ë¡œì›Œ ê¹Œë‹¤ë¡œì›€.
                # í¸ì˜ìƒ ì—„ì§€ íŒ(4)ì´ ì—„ì§€ ê¸°ì €ë¶€(2)ë³´ë‹¤ ìœ„ì— ìˆê±°ë‚˜ ë‹¨ìˆœíˆ í´ì¡ŒëŠ”ì§€ í™•ì¸.
                # ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•: ì—„ì§€ íŒ(4)ì˜ xì¢Œí‘œê°€ ìƒˆë¼ì†ê°€ë½ ìª½ì¸ì§€ ê²€ì§€ ìª½ì¸ì§€ íŒë‹¨.
                # ê±°ìš¸ëª¨ë“œ(Flip) ìƒíƒœ: ì˜¤ë¥¸ìª½ í™”ë©´ì´ ì˜¤ë¥¸ì†. 
                # ì˜¤ë¥¸ì†ì¼ ë•Œ: ì—„ì§€(4)ê°€ ê´€ì ˆ(3)ë³´ë‹¤ ì™¼ìª½(<)ì´ë©´ í´ì§„ ê²ƒ. (í™”ë©´ìƒ ì™¼ìª½ì´ ì‹¤ì œ ì˜¤ë¥¸ìª½)
                # ë³µì¡í•˜ë¯€ë¡œ ì—„ì§€ëŠ” yì¢Œí‘œê°€ ê´€ì ˆë³´ë‹¤ í™•ì‹¤íˆ ìœ„ì— ìˆëŠ”ì§€ë§Œ ì²´í¬í•˜ê±°ë‚˜, ì¼ë‹¨ ì œì™¸í•˜ê³  4ì†ê°€ë½ë§Œ ë³¼ ìˆ˜ë„ ìˆìŒ.
                # ì—¬ê¸°ì„œëŠ” 'ì—„ì§€ íŒì´ ê´€ì ˆ(3)ë³´ë‹¤ xì¢Œí‘œ ì°¨ì´ê°€ í¬ê±°ë‚˜' í•˜ëŠ” ì‹ìœ¼ë¡œ ë§ì´ í•˜ëŠ”ë°,
                # ì§ê´€ì ì¸ 'ì—„ì§€ ì²™'ì„ ìœ„í•´ ì—„ì§€ íŒì´ ê²€ì§€ ê´€ì ˆ(5)ë³´ë‹¤ ë©€ë¦¬ ë–¨ì–´ì ¸ìˆëŠ”ì§€ ë“±ìœ¼ë¡œ íŒë³„.
                # ì¼ë‹¨ ê°„ë‹¨í•œ ë¡œì§: ì—„ì§€ íŒì´ ê²€ì§€ ê´€ì ˆë³´ë‹¤ ë°”ê¹¥ìª½(ëª¸ ë°”ê¹¥)ì— ìˆìŒ.
                
                # ì—„ì§€ (ë‹¨ìˆœí™”: xì¢Œí‘œê°€ ê²€ì§€ ê´€ì ˆë³´ë‹¤ ë©€ë¦¬ ë–¨ì–´ì§)
                # ì˜¤ë¥¸ì†/ì™¼ì† êµ¬ë¶„ì´ ì—†ìœ¼ë©´ í—·ê°ˆë¦¼.
                # ì—„ì§€ëŠ” ì¼ë‹¨ ì œì™¸í•˜ê±°ë‚˜, ë‹¨ìˆœ yì¢Œí‘œë¡œ ë´…ë‹ˆë‹¤ (ìœ„ë¡œ ë“¤ì—ˆëŠ”ì§€).
                # ì—„ì§€ íŒ(4)ì˜ yê°€ ê²€ì§€ ê´€ì ˆ(5)ì˜ yë³´ë‹¤ ì‘ìœ¼ë©´ (ìœ„ì— ìˆìœ¼ë©´) Upìœ¼ë¡œ ê°„ì£¼? 
                # í•˜ì§€ë§Œ ì£¼ë¨¹ì¥˜ ë•Œë„ ê·¸ëŸ´ ìˆ˜ ìˆìŒ.
                # ì•ˆì „í•˜ê²Œ: ì—„ì§€ íŒ(4)ê³¼ ìƒˆë¼ íŒ(20)ì˜ ê±°ë¦¬ê°€ ë©€ë©´ í´ì§„ ê²ƒ?
                
                # ì—„ì§€ íŒë³„ ë¡œì§ (xì¢Œí‘œ ê¸°ë°˜, ì˜¤ë¥¸ì† ì¡ì´ ê°€ì • or hand label check)
                # ì—¬ê¸°ì„œëŠ” ì—„ì§€ ì œì™¸ 4ì†ê°€ë½ ìœ„ì£¼ë¡œ í•˜ê³ , ì—„ì§€ëŠ” ë³„ë„ ì œìŠ¤ì²˜ë¡œ ì·¨ê¸‰.
                
                # ë‚˜ë¨¸ì§€ 4ì†ê°€ë½ (ê²€ì§€~ì†Œì§€) : íŒì˜ yê°€ ê´€ì ˆ(Dip)ì˜ yë³´ë‹¤ ìœ„ì—(ì‘ê²Œ) ìˆìœ¼ë©´ í´ì§„ ê²ƒ
                # Landmark: Tip(8,12,16,20), PIP(6,10,14,18) - PIPë³´ë‹¤ íŒì´ ìœ„ì— ìˆì–´ì•¼ í´ì§„ ê²ƒ
                
                # ê²€ì§€
                if hand_landmarks.landmark[8].y < hand_landmarks.landmark[6].y:
                    fingers.append(1)
                else:
                    fingers.append(0)
                
                # ì¤‘ì§€
                if hand_landmarks.landmark[12].y < hand_landmarks.landmark[10].y:
                    fingers.append(1)
                else:
                    fingers.append(0)
                    
                # ì•½ì§€
                if hand_landmarks.landmark[16].y < hand_landmarks.landmark[14].y:
                    fingers.append(1)
                else:
                    fingers.append(0)
                    
                # ì†Œì§€
                if hand_landmarks.landmark[20].y < hand_landmarks.landmark[18].y:
                    fingers.append(1)
                else:
                    fingers.append(0)

                # ì—„ì§€ íŒë³„ (ì—„ì§€ ì²™ ì œìŠ¤ì²˜ìš©): ì—„ì§€ íŒ(4)ì´ ê²€ì§€ ê´€ì ˆ(6)ë³´ë‹¤ ìƒë‹¹íˆ ìœ„ì— ìˆê³ , ë‚˜ë¨¸ì§€ ì†ê°€ë½ì€ ì ‘í˜
                thumb_up = False
                if hand_landmarks.landmark[4].y < hand_landmarks.landmark[3].y and \
                   hand_landmarks.landmark[4].y < hand_landmarks.landmark[8].y:
                       thumb_up = True

                # ì œìŠ¤ì²˜ ì¸ì‹
                # fingers = [ê²€ì§€, ì¤‘ì§€, ì•½ì§€, ì†Œì§€]
                
                cx, cy = int(hand_landmarks.landmark[8].x * width), int(hand_landmarks.landmark[8].y * height)

                # 1. Fist (All Down) -> Clear
                if fingers == [0, 0, 0, 0] and not thumb_up:
                    gesture_name = "Fist (Clear)"
                    curr_time = time.time()
                    if curr_time - last_action_time > cooldown:
                        canvas = np.zeros_like(frame)
                        prediction_text = ""
                        last_action_time = curr_time
                        print("Canvas Cleared via Gesture")
                    x1, y1 = 0, 0

                # 2. Rock (Index + Pinky Up) -> Switch Mode
                elif fingers == [1, 0, 0, 1]:
                    gesture_name = "Rock (Switch)"
                    curr_time = time.time()
                    if curr_time - last_action_time > cooldown:
                        current_mode = "alpha" if current_mode == "digit" else "digit"
                        prediction_text = ""
                        last_action_time = curr_time
                        print(f"Switched to {current_mode} via Gesture")
                    x1, y1 = 0, 0

                # 3. Two Fingers (Index + Middle) -> Hover (Move without drawing)
                elif fingers == [1, 1, 0, 0]:
                    gesture_name = "Hover"
                    cv2.circle(frame, (cx, cy), 15, (255, 0, 255), 2) # ì»¤ì„œ í‘œì‹œ
                    x1, y1 = 0, 0 # ì„  ëŠê¸°

                # 4. Only Index Up -> Draw
                elif fingers == [1, 0, 0, 0]:
                    gesture_name = "Draw"
                    cv2.circle(frame, (cx, cy), 15, (0, 255, 255), cv2.FILLED)
                    if x1 == 0 and y1 == 0:
                        x1, y1 = cx, cy
                    else:
                        cv2.line(canvas, (x1, y1), (cx, cy), (255, 255, 255), 15)
                        x1, y1 = cx, cy

                # 5. Thumb Up (Strict check: others down) -> Predict
                elif thumb_up and fingers == [0, 0, 0, 0]:
                    gesture_name = "Thumb Up (Predict)"
                    curr_time = time.time()
                    if curr_time - last_action_time > cooldown:
                        # ì˜ˆì¸¡ ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
                        gray_canvas = cv2.cvtColor(canvas, cv2.COLOR_BGR2GRAY)
                        contours_canvas, _ = cv2.findContours(gray_canvas, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                        
                        if len(contours_canvas) > 0:
                            x, y, w, h = cv2.boundingRect(np.vstack(contours_canvas))
                            padding = 20
                            x = max(0, x - padding)
                            y = max(0, y - padding)
                            w = min(canvas.shape[1] - x, w + 2 * padding)
                            h = min(canvas.shape[0] - y, h + 2 * padding)
                            
                            roi = gray_canvas[y:y+h, x:x+w]
                            roi_pil = Image.fromarray(roi)
                            max_dim = max(roi_pil.size)
                            new_img = Image.new("L", (max_dim, max_dim), 0)
                            new_img.paste(roi_pil, ((max_dim - roi_pil.width) // 2, (max_dim - roi_pil.height) // 2))
                            new_img = new_img.resize((28, 28), Image.Resampling.BICUBIC)
                            
                            with torch.no_grad():
                                if current_mode == "digit" and models_loaded["digit"]:
                                    img_tensor = transform(new_img).unsqueeze(0).to(device)
                                    output = digit_model(img_tensor)
                                    probabilities = torch.nn.functional.softmax(output, dim=1)
                                    confidence, predicted = torch.max(probabilities, 1)
                                    res_char = str(predicted.item())
                                elif current_mode == "alpha" and models_loaded["alpha"]:
                                    img_tensor = emnist_transform_fn(new_img).unsqueeze(0).to(device)
                                    output = alpha_model(img_tensor)
                                    probabilities = torch.nn.functional.softmax(output, dim=1)
                                    confidence, predicted = torch.max(probabilities, 1)
                                    res_char = alphabet_map[predicted.item()]
                                else:
                                    res_char = "Err"
                                    confidence = torch.tensor(0.0)

                            prediction_text = f"{res_char} ({confidence.item()*100:.1f}%)"
                            print(f"Predicted via Gesture: {prediction_text}")
                        last_action_time = curr_time
                    x1, y1 = 0, 0
                else:
                    # ê·¸ ì™¸ ì œìŠ¤ì²˜
                    x1, y1 = 0, 0

        # ìº”ë²„ìŠ¤ì™€ í”„ë ˆì„ í•©ì„±
        frame = cv2.add(frame, canvas)
        
        # UI í…ìŠ¤íŠ¸
        mode_color = (0, 255, 0) if current_mode == "digit" else (255, 0, 255)
        cv2.Rectangle = cv2.rectangle(frame, (0,0), (width, 80), (0,0,0), -1) # ìƒë‹¨ ë¸”ë™ ë°” ë°°ê²½
        cv2.putText(frame, f"MODE: {current_mode.upper()}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, mode_color, 2)
        cv2.putText(frame, f"Gesture: {gesture_name}", (10, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)
        
        if prediction_text:
             cv2.putText(frame, f"Prediction: {prediction_text}", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)

        cv2.imshow("Hand Tracking Canvas", frame)

        key = cv2.waitKey(1) & 0xFF
        
        if key == ord("q"):
            break
        elif key == ord("c"):
            canvas = np.zeros_like(frame)
            prediction_text = ""
        elif key == ord("m"):
            current_mode = "alpha" if current_mode == "digit" else "digit"
            prediction_text = ""
        elif key == ord("p"):
            pass # í‚¤ë³´ë“œ ì˜ˆì¸¡ì€ ìœ ì§€í•˜ê±°ë‚˜ ì œìŠ¤ì²˜ë‘ ë³‘í–‰

    cap.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()
